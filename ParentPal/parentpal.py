# -*- coding: utf-8 -*-
"""ParentPal.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DbrnOTrHjIi-OEtKg-Or6auydmBYgCmB
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, LabelEncoder
from sklearn.metrics import accuracy_score
from keras.models import Sequential
from keras.layers import Dense, Dropout, BatchNormalization

data = pd.read_csv('Dataset Fix.csv')
data_visual = data

data.head()

# Encode the categorical labels as integers.
#
# Details:
# This stage is necessary if your classification label is represented as a
# string since Keras expects integer classification labels.
# When using `pd_dataframe_to_tf_dataset` (see below), this step can be skipped.

# Name of the label column.
label = "Status"

classes = data[label].unique().tolist()
print(f"Label classes: {classes}")

data[label] = data[label].map(classes.index)
# Define features (X) and target variable (y)

X = data.iloc[:, 0:4].values
y = data.iloc[:, 4].values

X

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# # Scale features using MinMaxScaler
# scaler = MinMaxScaler()
# X_train = scaler.fit_transform(X_train)
# X_test = scaler.transform(X_test)

X_train

# Define the neural network model
model = Sequential()
model.add(Dense(64, input_dim=4, activation='relu')) # input layer with 4 neurons (Sex, Age, Height, Weight)
model.add(Dense(64, activation='relu')) # hidden layer with 64 neurons
model.add(Dense(32, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(16, activation='relu'))
model.add(Dense(4, activation='softmax')) # output layer with 4 neurons (one for each status)

# Compile the model
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, epochs=1000, batch_size=32, validation_split=0.2)

# evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print('Test Accuracy:', accuracy)

# Make predictions
predictions = model.predict([[1, 0, 54, 4]])
predictions
# Label classes: ['Gizi Baik', 'Overweight', 'Underweight', 'Stunting']


# # Get the index of the highest probability
# predictions = np.argmax(predictions, axis=1)
# # Print predictions
# print(predictions)

# import numpy as np

# # Assuming you have a new set of data in a DataFrame named 'new_data'
# # Make sure the columns are in the same order as X_train (['Sex', 'Age', 'Height', 'Weight'])

# new_data = pd.DataFrame({
#     'Sex': [1],
#     'Age': [1],
#     'Height': [81],
#     'Weight': [10]
# })

# # # Scale features using the same MinMaxScaler instance
# # new_data_scaled = scaler.transform(new_data[['Sex', 'Age', 'Height', 'Weight']])

# # Make predictions
# predictions = model.predict(new_data)

# # Get the predicted class for each instance
# predicted_label_index = np.argmax(predictions)

# # Map back to the original label
# predicted_label = classes[predicted_label_index]

# print("Status Anak:", predicted_label)

#Generate a Saved Model
export_dir = 'saved_model/4'
tf.saved_model.save(model, export_dir)

# Convert the model.
converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)
tflite_model = converter.convert()

# Save the TF Lite model.
with tf.io.gfile.GFile('pp.tflite', 'wb') as f:
  f.write(tflite_model)

